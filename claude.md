# Metadata Schema Tools - Implementation Summary

## Overview

Metadata schemas are defined as **LinkML YAML** files under `linkml/`.
From these single-source definitions the toolchain generates:

- **JSON Schema** files (`schemas/*.json`) — used for validation and enrichment
- **Pydantic models** (`mdstools/models/*.py`) — used for typed validation in Python

Package schemas (`echemdb_package.json`, `svgdigitizer_package.json`) compose
with the **Frictionless Data Package** standard via `allOf` references to
locally cached Frictionless schemas (`schemas/frictionless/`, gitignored,
downloaded on demand by `ensure_frictionless_schemas()`).

Flattened metadata can be exported to Excel/CSV with descriptions and examples
pulled from the JSON schemas (the "enrichment" workflow).

## What We Built

### 1. Core Modules (mdstools/)

#### `metadata/` Package
- **metadata.py**: `Metadata` class for working with nested YAML/dict structures
- **flattened_metadata.py**: `FlattenedMetadata` class for tabular [number, key, value] format
- **enriched_metadata.py**: `EnrichedFlattenedMetadata` class with schema-based enrichment

**Key Features**:
- Flatten nested dicts and lists with hierarchical numbering (1, 1.1, 1.1.a, etc.)
- Export to CSV, Excel, and Markdown
- Schema-based enrichment with descriptions and examples

#### `converters/` Package
- **flatten.py**: Convert nested structures to tabular rows
- **unflatten.py**: Reconstruct nested structures from tabular rows

#### `schema/` Package
- **enricher.py**: `SchemaEnricher` class — handles both `$defs` (LinkML) and `definitions` (legacy) JSON Schema formats
- **generate_from_linkml.py**: Generate JSON Schemas and Pydantic models from LinkML; includes `ensure_frictionless_schemas()` download helper and Frictionless composition post-processing for package schemas
- **validate_examples.py**: Validation of example files against schemas (objects, file schemas, and package schemas with local Frictionless registry)
- **check_naming.py**: Enforce naming conventions (snake_case files, PascalCase definitions, camelCase properties)
- **validator.py**: Schema validation — both JSON Schema and Pydantic-based
- **update_expected_schemas.py**: Script to update expected schema snapshots

#### `models/` Package (auto-generated)
- **minimum_echemdb.py**, **autotag.py**, **source_data.py**, **svgdigitizer.py**, **echemdb_package.py**, **svgdigitizer_package.py**
- Pydantic models with `extra="allow"`, `coerce_numbers_to_str=True`
- Generated by `gen-pydantic` from LinkML + post-processing

### 2. CLI Interface

#### `entrypoint.py`
- **Purpose**: Click-based command-line interface for YAML to Excel/CSV conversion and back
- **Entry point**: Registered as `mdstools` via `[project.scripts]` in `pyproject.toml`
- **Commands**: `flatten` and `unflatten`
- **Usage**: `mdstools flatten <yaml_file> [options]` or `pixi run flatten <yaml_file> [options]`
- **Features**: Schema enrichment, multiple output formats, configurable paths
- **Pattern**: Follows the same structure as `unitpackage/entrypoint.py` — `@click.group` with `@click.command` subcommands added via `cli.add_command()`
- **Doctest support**: `__test__` dict re-exports command docstrings for pytest-doctestplus discovery

### 3. Testing

#### `test/cli.py`
- `invoke()` helper wrapping Click's `CliRunner` for clean doctest usage
- Adapted from `unitpackage/test/cli.py`

#### `test/test_comprehensive.py`
- Complete test suite covering all functionality
- Tests: basic flattening, enrichment, multi-sheet export, field-specific enrichment, markdown export
- Generates test outputs in `tests/generated/`

#### `test/test_resolved_schemas.py`
- Snapshot tests comparing resolved schemas against `schemas/expected/`

## How It Works

### The Workflow

1. **Input**: User provides nested YAML/dict data
2. **Flattening**: Convert to numbered tabular structure
3. **Schema Lookup**: For each field, look up its schema definition
4. **Enrichment**: Add Description and Example columns from schema
5. **Export**: Generate Excel/CSV files with enriched metadata

### Example

**Input YAML:**
```yaml
curation:
  process:
    - role: curator
      name: Jane Doe
```

**Flattened + Enriched:**
| Number  | Key  | Value     | Example        | Description                    |
|---------|------|-----------|----------------|--------------------------------|
| 1       | curation | <nested> |            |                                |
| 1.1     | process | <nested> |            | List of people involved...     |
| 1.1.a   |      | <nested> |                |                                |
| 1.1.a.1 | role | curator   | experimentalist| A person that recorded...      |
| 1.1.a.2 | name | Jane Doe  | Jane Doe       | Full name of the person.       |

### Schema Generation (LinkML)

**Single source of truth**: All metadata schemas are defined as LinkML YAML files under `linkml/`.

**Generation pipeline** (`mdstools/schema/generate_from_linkml.py`):
1. `ensure_frictionless_schemas()` — downloads Frictionless `datapackage.json` and `dataresource.json` to `schemas/frictionless/` if not already present (gitignored, fetched on demand)
2. `gen-json-schema <linkml_file>` → JSON Schema (with `$defs`, self-contained)
3. Post-processing: add `$schema`/`$id`, fix Quantity value/unit types, add fieldMapping
4. Post-processing (package schemas only): compose resource items with Frictionless `dataresource.json` via `allOf`, set `additionalProperties: true` on Package/Resource defs
5. `gen-pydantic <linkml_file>` → Pydantic models
6. Post-processing: replace `extra="forbid"` with `extra="allow"`, add `coerce_numbers_to_str`

**Commands**:
```bash
pixi run generate-schemas         # Generate JSON Schemas from LinkML
pixi run generate-models          # Generate Pydantic models from LinkML
pixi run generate-all             # Generate both
pixi run resolve-schemas          # Alias for generate-schemas
pixi run update-expected-schemas  # Update expected baselines after intentional changes
```

**Note**: The enricher handles both modern `$defs` and legacy `definitions` JSON Schema formats.

### Generated Schemas

The following schemas are generated from `linkml/` into `schemas/`:
- **autotag.json** - Complete echemdb metadata for auto-generated YAML
- **minimum_echemdb.json** - Minimum metadata for echemdb
- **source_data.json** - Source data with data description (dialect, field mapping, field units)
- **svgdigitizer.json** - Digitizer output metadata
- **echemdb_package.json** - Data package for echemdb
- **svgdigitizer_package.json** - Data package for svgdigitizer

## Key Design Decisions

### Why Two Separate Modules?
- **Separation of Concerns**: Flattening logic is independent of schema enrichment
- **Flexibility**: Users can use flattening without schemas, or enrichment for other purposes
- **Maintainability**: Easier to test and modify each component independently

### Why On-the-Fly $ref Resolution?
- **Simplicity**: No need to maintain pre-resolved schema files
- **Cleaner Repo**: Avoid committing generated files
- **Flexibility**: Works with any schema structure automatically

### Why LinkML as Single Source of Truth?
- **One definition, many outputs**: JSON Schema, Pydantic, SHACL, etc. from one YAML
- **Modular imports**: LinkML supports cross-file imports matching the existing piece-based structure
- **Rich metadata**: descriptions, examples, enums, constraints all in one place
- **Future-proof**: Ontology URIs (w3id.org) can be added incrementally without breaking anything

### Why Hierarchical Numbering (1.1.a.1)?
- **Human Readable**: Easy to understand nesting depth
- **Excel Friendly**: Can be sorted naturally
- **Reconstruction**: Numbering preserves structure for future unflattening

## File Structure

```
metadata-schema/
├── linkml/                        # LinkML YAML schemas (single source of truth)
│   ├── minimum_echemdb.yaml      # Main models (6 root schemas)
│   ├── autotag.yaml
│   ├── source_data.yaml
│   ├── svgdigitizer.yaml
│   ├── echemdb_package.yaml
│   ├── svgdigitizer_package.yaml
│   ├── curation.yaml             # Shared schema components
│   ├── source.yaml
│   ├── eln.yaml
│   ├── experimental.yaml
│   ├── figure_description.yaml
│   ├── projects.yaml
│   ├── system.yaml
│   ├── data_description.yaml
│   ├── general/                  # Reusable types
│   │   ├── quantity.yaml
│   │   ├── url.yaml
│   │   ├── purity.yaml
│   │   ├── chemical_identifiers.yaml
│   │   └── component.yaml
│   ├── system/                   # System sub-components
│   │   ├── atmosphere.yaml
│   │   ├── electrode.yaml
│   │   ├── electrolyte.yaml
│   │   └── electrochemical_cell.yaml
│   └── experimental/
│       └── instrumentation.yaml
│
├── mdstools/                      # Main package
│   ├── __init__.py
│   ├── entrypoint.py             # Click-based CLI (entry point: mdstools)
│   ├── metadata/                 # Metadata classes
│   │   ├── metadata.py           # Nested dict/YAML wrapper
│   │   ├── flattened_metadata.py # Tabular format wrapper
│   │   └── enriched_metadata.py  # Schema-enriched wrapper
│   ├── converters/               # Conversion functions
│   │   ├── flatten.py            # Nested → tabular
│   │   └── unflatten.py          # Tabular → nested
│   ├── schema/                   # Schema utilities
│   │   ├── enricher.py           # Schema-based enrichment ($defs + definitions)
│   │   ├── generate_from_linkml.py # Generate JSON Schema + Pydantic from LinkML
│   │   ├── validate_examples.py  # Validate examples against schemas (objects, files, packages)
│   │   ├── check_naming.py       # Enforce naming conventions
│   │   ├── validator.py          # JSON Schema + Pydantic validation
│   │   └── update_expected_schemas.py  # Update expected snapshots
│   ├── models/                   # Auto-generated Pydantic models
│   │   ├── __init__.py
│   │   ├── minimum_echemdb.py
│   │   ├── autotag.py
│   │   ├── source_data.py
│   │   ├── svgdigitizer.py
│   │   ├── echemdb_package.py
│   │   └── svgdigitizer_package.py
│   └── test/                     # Test files and helpers
│       ├── cli.py                # Click CliRunner helper (invoke())
│       ├── test_comprehensive.py # Full test suite
│       └── test_resolved_schemas.py  # Snapshot tests for generated schemas
│
├── schemas/                       # Resolved JSON Schema files
│   ├── autotag.json
│   ├── minimum_echemdb.json
│   ├── source_data.json
│   ├── svgdigitizer.json
│   ├── echemdb_package.json
│   ├── svgdigitizer_package.json
│   ├── frictionless/             # Frictionless standard schemas (gitignored, auto-downloaded)
│   │   ├── datapackage.json
│   │   └── dataresource.json
│   ├── expected/                 # Expected baselines for snapshot testing
│   │   └── *.json
│
├── examples/                      # Example YAML/JSON files
│   ├── file_schemas/             # Examples per schema type
│   │   ├── autotag.yaml
│   │   ├── minimum_echemdb.yaml
│   │   ├── source_data.yaml
│   │   ├── svgdigitizer.yaml
│   │   ├── echemdb_package.json
│   │   └── svgdigitizer_package.json
│   └── objects/                  # Examples of individual objects
│
├── tests/                         # Test data
│   ├── simple_test.yaml
│   ├── example_metadata.yaml
│   └── from_csv_example.csv
│
└── generated/                     # Project outputs (gitignored)
```

## Usage Examples

### Basic Flattening
```python
from mdstools.metadata.metadata import Metadata

metadata = Metadata(data)
flattened = metadata.flatten()
flattened.to_excel('output.xlsx')
```

### With Enrichment
```python
from mdstools.metadata.enriched_metadata import EnrichedFlattenedMetadata

metadata = Metadata(data)
flattened = metadata.flatten()
enriched = EnrichedFlattenedMetadata(flattened.rows, schema_dir='schemas')
enriched.to_excel('output.xlsx')
```

### Multi-Sheet Excel
```python
enriched.to_excel('output.xlsx', separate_sheets=True)
```

### Using CLI
```bash
mdstools flatten tests/example_metadata.yaml --output-dir generated
# or via pixi:
pixi run flatten tests/example_metadata.yaml --output-dir generated
```

## Testing

### Run All Tests
```bash
pixi run test                # Run all tests (doctests + comprehensive + resolved schemas)
pixi run doctest             # Run doctests only
pixi run test-comprehensive  # Run integration tests only
pixi run test-resolved-schemas  # Run resolved schema snapshot tests
```

### Resolve & Validate Schemas
```bash
pixi run resolve-schemas         # Generate schemas from LinkML
pixi run generate-schemas        # Same as resolve-schemas
pixi run generate-models         # Generate Pydantic models from LinkML
pixi run generate-all            # Generate both schemas and models
pixi run update-expected-schemas # Update expected baselines after intentional changes
pixi run validate                # Validate example YAMLs against schemas
pixi run diff-schemas            # Show diffs between expected and resolved schemas
```

### Run Conversion
```bash
mdstools flatten tests/example_metadata.yaml
# or via pixi:
pixi run flatten tests/example_metadata.yaml
```

## Possible Extensions

### Phase 3: Ontology URI Mappings (Deferred)
- Add `class_uri` and `slot_uri` to LinkML classes and slots using w3id.org URIs
- Enables semantic interoperability and linked-data export (RDF, JSON-LD)
- Can be done incrementally per schema piece without breaking anything
- Example: `Electrode` → `class_uri: w3id:echemdb/Electrode`

### SHACL / RDF Generation
- LinkML can generate SHACL shapes and RDF context from the same YAML definitions
- `gen-shacl`, `gen-jsonld-context`, `gen-rdf` are available LinkML generators
- Would allow validating metadata as linked data graphs
- Enables publishing metadata to SPARQL endpoints

### Enrichment Coverage Improvement
Currently ~14% on test data — limited by how many `description`/`examples` are in the LinkML schemas:
- Systematically add `description` to every slot in all LinkML YAML files
- Add `examples` values for all primitive fields
- Add `title` for human-friendly property names
- This is purely additive work — no code changes needed, just LinkML YAML edits
- Regenerate with `pixi run generate-all` after each batch of additions

### Excel Validation & Templates
- **Auto-completion dropdowns**: Generate Excel Data Validation lists from enum values in schemas
- **Pre-populated templates**: Create Excel templates with Example values as placeholder/lighter text
- **On-import validation**: Validate Excel cell values against schema constraints (type, enum, pattern) during unflatten
- **Conditional formatting**: Highlight required fields, mark invalid values in red

### Schema Versioning & Distribution
- Tag schema versions with `version:` in LinkML YAML
- Include generated JSON schemas in GitHub release assets for easy download
- Publish schemas to a public URL (e.g., `https://echemdb.github.io/metadata-schema/schemas/autotag.json`)
- Add `$id` URLs pointing to the published location

### Additional CLI Commands
- `mdstools validate <yaml_file> --schema <schema_name>` — validate YAML against a schema
- `mdstools template <schema_name>` — generate a blank YAML template from a schema
- `mdstools diff <file1> <file2>` — semantic diff between two metadata YAML files
- `mdstools lint <yaml_file>` — check metadata for best practices beyond schema compliance

### Cross-Project Integration
- **unitpackage**: Use Pydantic models for metadata validation in unitpackage data packages
- **echemdb website**: Validate contributed metadata before ingestion
- **svgdigitizer**: Validate digitizer output metadata on export

### Schema Composition & Profiles
- Define "profiles" as LinkML subsets (e.g., a "quick entry" profile with only required fields)
- Generate profile-specific Excel templates with reduced column sets
- Allow users to select a profile in the CLI: `mdstools flatten --profile quick`

### Documentation Generation
- Generate human-readable schema documentation from LinkML (`gen-doc`)
- Publish as part of the project website
- Auto-generate field reference tables for each schema type
- Include diagrams of schema relationships (class hierarchy, slot reuse)

## Testing & Maintenance Notes

### Test Structure
- `pixi run test` - Runs all tests (doctests + comprehensive + resolved schema snapshots)
- `pixi run doctest` - Runs doctests in mdstools modules
- `pixi run test-comprehensive` - Runs integration tests (6 tests)
- `pixi run test-resolved-schemas` - Snapshot tests comparing resolved schemas against `schemas/expected/`
- Test outputs go to `tests/generated/` (gitignored)

### CLI Usage
- `mdstools flatten <yaml_file>` / `pixi run flatten <yaml_file>` - Flatten YAML to Excel/CSV
- `mdstools unflatten <file>` / `pixi run unflatten <file>` - Unflatten Excel/CSV back to YAML
- Click-based CLI registered as `mdstools` entry point via `[project.scripts]`
- Follows unitpackage's entrypoint pattern (`@click.group` + `@click.command` + `cli.add_command()`)
- Doctests in command docstrings discovered via `__test__` dict and pytest-doctestplus
- Outputs to `generated/` directory by default

### File Organization Decisions
- **Generated folders**:
  - `/generated` - Project outputs (user-facing conversions)
  - `/tests/generated` - Test outputs
  - Both excluded via `.gitignore`
- **External schemas**:
  - `schemas/frictionless/` - Frictionless Data Package standard schemas, gitignored
  - Auto-downloaded by `ensure_frictionless_schemas()` on first schema generation or package validation
- **Test files**:
  - `tests/simple_test.yaml` - Small test data for automated tests
  - `tests/example_metadata.yaml` - Comprehensive example for demos
- **Resolved schemas**: Generated from LinkML into `schemas/` and checked against `schemas/expected/` in CI
  - `mdstools/schema/generate_from_linkml.py` is the main generator
  - `mdstools/schema/update_expected_schemas.py` updates expected baselines
  - Schemas generated: autotag, minimum_echemdb, source_data, svgdigitizer, echemdb_package, svgdigitizer_package

## Known Limitations

1. **Enrichment Coverage**: Currently ~14% on test data
   - Limited by how many descriptions/examples are in the JSON schemas
   - Not a code limitation - add more to schemas to improve

2. **Array Item Handling**:
   - Array items get lettered identifiers (1.1.a, 1.1.b)
   - Description/Example come from the array schema, not individual items
   - Works correctly but could be confusing for users

3. **Excel Limitations**:
   - Very large nested structures may create unwieldy Excel sheets
   - No built-in validation yet (users can enter invalid data)

## Conclusion

This implementation successfully provides:
1. ✅ Robust YAML ↔ Tabular conversion
2. ✅ Schema-based enrichment with descriptions/examples
3. ✅ Multiple export formats (CSV, Excel single/multi-sheet, Markdown)
4. ✅ Comprehensive test coverage (72 tests total: 65 doctests + 6 comprehensive + 1 snapshot)
5. ✅ Clean, documented, maintainable code
6. ✅ CLI interface via pixi tasks
7. ✅ Proper file organization and gitignore setup
8. ✅ Unflatten + validation (Excel/CSV → YAML with schema validation)
9. ✅ Schema snapshot testing (generated schemas vs expected baselines)
10. ✅ Data description schema (dialect, field mapping, field units) for source data files
11. ✅ LinkML as single source of truth (JSON Schema + Pydantic generated)
12. ✅ Pydantic-based validation with `validate_with_pydantic()`
13. ✅ Frictionless Data Package composition (package schemas compose with Frictionless via `allOf`)
14. ✅ On-demand Frictionless schema download (`ensure_frictionless_schemas()`, gitignored)

The system is ready for users to:
- Generate Excel templates from YAML examples
- Fill out metadata with helpful descriptions and examples
- Convert completed Excel sheets back to YAML
- Validate metadata against JSON schemas

Schema types supported:
- **autotag** - Complete auto-generated echemdb metadata
- **minimum_echemdb** - Minimum set for electrochemical data
- **source_data** - Source data files with data description (dialect, field mapping, units)
- **svgdigitizer** - Digitizer output metadata
- **echemdb_package / svgdigitizer_package** - Data packages
