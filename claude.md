# Metadata Schema Tools - Implementation Summary

## Overview

Metadata schemas are defined as **LinkML YAML** files under `linkml/`.
From these single-source definitions the toolchain generates:

- **JSON Schema** files (`schemas/*.json`) — used for validation and enrichment
- **Pydantic models** (`mdstools/models/*.py`) — used for typed validation in Python

Flattened metadata can be exported to Excel/CSV with descriptions and examples
pulled from the JSON schemas (the "enrichment" workflow).

## What We Built

### 1. Core Modules (mdstools/)

#### `metadata/` Package
- **metadata.py**: `Metadata` class for working with nested YAML/dict structures
- **flattened_metadata.py**: `FlattenedMetadata` class for tabular [number, key, value] format
- **enriched_metadata.py**: `EnrichedFlattenedMetadata` class with schema-based enrichment

**Key Features**:
- Flatten nested dicts and lists with hierarchical numbering (1, 1.1, 1.1.a, etc.)
- Export to CSV, Excel, and Markdown
- Schema-based enrichment with descriptions and examples

#### `converters/` Package
- **flatten.py**: Convert nested structures to tabular rows
- **unflatten.py**: Reconstruct nested structures from tabular rows

#### `schema/` Package
- **enricher.py**: `SchemaEnricher` class — handles both `$defs` (LinkML) and `definitions` (legacy) JSON Schema formats
- **generate_from_linkml.py**: Generate JSON Schemas and Pydantic models from LinkML
- **resolver.py**: Legacy `SchemaResolver` for resolving `$ref` from YAML schema pieces (kept for reference)
- **validator.py**: Schema validation — both JSON Schema and Pydantic-based
- **update_expected_schemas.py**: Script to update expected schema snapshots

#### `models/` Package (auto-generated)
- **minimum_echemdb.py**, **autotag.py**, **source_data.py**, **svgdigitizer.py**, **echemdb_package.py**, **svgdigitizer_package.py**
- Pydantic models with `extra="allow"`, `coerce_numbers_to_str=True`
- Generated by `gen-pydantic` from LinkML + post-processing

### 2. CLI Interface

#### `entrypoint.py`
- **Purpose**: Click-based command-line interface for YAML to Excel/CSV conversion and back
- **Entry point**: Registered as `mdstools` via `[project.scripts]` in `pyproject.toml`
- **Commands**: `flatten` and `unflatten`
- **Usage**: `mdstools flatten <yaml_file> [options]` or `pixi run flatten <yaml_file> [options]`
- **Features**: Schema enrichment, multiple output formats, configurable paths
- **Pattern**: Follows the same structure as `unitpackage/entrypoint.py` — `@click.group` with `@click.command` subcommands added via `cli.add_command()`
- **Doctest support**: `__test__` dict re-exports command docstrings for pytest-doctestplus discovery

### 3. Testing

#### `test/cli.py`
- `invoke()` helper wrapping Click's `CliRunner` for clean doctest usage
- Adapted from `unitpackage/test/cli.py`

#### `test/test_comprehensive.py`
- Complete test suite covering all functionality
- Tests: basic flattening, enrichment, multi-sheet export, field-specific enrichment, markdown export
- Generates test outputs in `tests/generated/`

#### `test/test_resolved_schemas.py`
- Snapshot tests comparing resolved schemas against `schemas/expected/`

## How It Works

### The Workflow

1. **Input**: User provides nested YAML/dict data
2. **Flattening**: Convert to numbered tabular structure
3. **Schema Lookup**: For each field, look up its schema definition
4. **Enrichment**: Add Description and Example columns from schema
5. **Export**: Generate Excel/CSV files with enriched metadata

### Example

**Input YAML:**
```yaml
curation:
  process:
    - role: curator
      name: Jane Doe
```

**Flattened + Enriched:**
| Number  | Key  | Value     | Example        | Description                    |
|---------|------|-----------|----------------|--------------------------------|
| 1       | curation | <nested> |            |                                |
| 1.1     | process | <nested> |            | List of people involved...     |
| 1.1.a   |      | <nested> |                |                                |
| 1.1.a.1 | role | curator   | experimentalist| A person that recorded...      |
| 1.1.a.2 | name | Jane Doe  | Jane Doe       | Full name of the person.       |

### Schema Generation (LinkML)

**Single source of truth**: All metadata schemas are defined as LinkML YAML files under `linkml/`.

**Generation pipeline** (`mdstools/schema/generate_from_linkml.py`):
1. `gen-json-schema <linkml_file>` → JSON Schema (with `$defs`, self-contained)
2. Post-processing: add `$schema`/`$id`, fix Quantity value/unit types, add fieldMapping
3. `gen-pydantic <linkml_file>` → Pydantic models
4. Post-processing: replace `extra="forbid"` with `extra="allow"`, add `coerce_numbers_to_str`

**Commands**:
```bash
pixi run generate-schemas         # Generate JSON Schemas from LinkML
pixi run generate-models          # Generate Pydantic models from LinkML
pixi run generate-all             # Generate both
pixi run resolve-schemas          # Alias for generate-schemas
pixi run update-expected-schemas  # Update expected baselines after intentional changes
```

**Note**: The enricher handles both modern `$defs` and legacy `definitions` JSON Schema formats.

### Generated Schemas

The following schemas are generated from `linkml/` into `schemas/`:
- **autotag.json** - Complete echemdb metadata for auto-generated YAML
- **minimum_echemdb.json** - Minimum metadata for echemdb
- **source_data.json** - Source data with data description (dialect, field mapping, field units)
- **svgdigitizer.json** - Digitizer output metadata
- **echemdb_package.json** - Data package for echemdb
- **svgdigitizer_package.json** - Data package for svgdigitizer

## Key Design Decisions

### Why Two Separate Modules?
- **Separation of Concerns**: Flattening logic is independent of schema enrichment
- **Flexibility**: Users can use flattening without schemas, or enrichment for other purposes
- **Maintainability**: Easier to test and modify each component independently

### Why On-the-Fly $ref Resolution?
- **Simplicity**: No need to maintain pre-resolved schema files
- **Cleaner Repo**: Avoid committing generated files
- **Flexibility**: Works with any schema structure automatically
- **Note**: `mdstools/schema/resolver.py` still available for creating distributable single-file schemas for releases

### Why LinkML as Single Source of Truth?
- **One definition, many outputs**: JSON Schema, Pydantic, SHACL, etc. from one YAML
- **Modular imports**: LinkML supports cross-file imports matching the existing piece-based structure
- **Rich metadata**: descriptions, examples, enums, constraints all in one place
- **Future-proof**: Ontology URIs (w3id.org) can be added incrementally without breaking anything

### Why Hierarchical Numbering (1.1.a.1)?
- **Human Readable**: Easy to understand nesting depth
- **Excel Friendly**: Can be sorted naturally
- **Reconstruction**: Numbering preserves structure for future unflattening

## File Structure

```
metadata-schema/
├── linkml/                        # LinkML YAML schemas (single source of truth)
│   ├── minimum_echemdb.yaml      # Main models (6 root schemas)
│   ├── autotag.yaml
│   ├── source_data.yaml
│   ├── svgdigitizer.yaml
│   ├── echemdb_package.yaml
│   ├── svgdigitizer_package.yaml
│   ├── curation.yaml             # Shared schema pieces
│   ├── source.yaml
│   ├── eln.yaml
│   ├── experimental.yaml
│   ├── figure_description.yaml
│   ├── projects.yaml
│   ├── system.yaml
│   ├── data_description.yaml
│   ├── general/                  # Reusable types
│   │   ├── quantity.yaml
│   │   ├── url.yaml
│   │   ├── purity.yaml
│   │   ├── chemical_identifiers.yaml
│   │   └── component.yaml
│   ├── system/                   # System sub-components
│   │   ├── atmosphere.yaml
│   │   ├── electrode.yaml
│   │   ├── electrolyte.yaml
│   │   └── electrochemical_cell.yaml
│   └── experimental/
│       └── instrumentation.yaml
│
├── mdstools/                      # Main package
│   ├── __init__.py
│   ├── entrypoint.py             # Click-based CLI (entry point: mdstools)
│   ├── metadata/                 # Metadata classes
│   │   ├── metadata.py           # Nested dict/YAML wrapper
│   │   ├── flattened_metadata.py # Tabular format wrapper
│   │   └── enriched_metadata.py  # Schema-enriched wrapper
│   ├── converters/               # Conversion functions
│   │   ├── flatten.py            # Nested → tabular
│   │   └── unflatten.py          # Tabular → nested
│   ├── schema/                   # Schema utilities
│   │   ├── enricher.py           # Schema-based enrichment ($defs + definitions)
│   │   ├── generate_from_linkml.py # Generate JSON Schema + Pydantic from LinkML
│   │   ├── resolver.py           # Legacy schema resolution (kept for reference)
│   │   ├── validator.py          # JSON Schema + Pydantic validation
│   │   └── update_expected_schemas.py  # Update expected snapshots
│   ├── models/                   # Auto-generated Pydantic models
│   │   ├── __init__.py
│   │   ├── minimum_echemdb.py
│   │   ├── autotag.py
│   │   ├── source_data.py
│   │   ├── svgdigitizer.py
│   │   ├── echemdb_package.py
│   │   └── svgdigitizer_package.py
│   └── test/                     # Test files and helpers
│       ├── cli.py                # Click CliRunner helper (invoke())
│       ├── test_comprehensive.py # Full test suite
│       └── test_resolved_schemas.py  # Snapshot tests for generated schemas
│
├── schemas/                       # Resolved JSON Schema files
│   ├── autotag.json
│   ├── minimum_echemdb.json
│   ├── source_data.json
│   ├── svgdigitizer.json
│   ├── echemdb_package.json
│   ├── svgdigitizer_package.json
│   ├── expected/                 # Expected baselines for snapshot testing
│   │   └── *.json
│   └── schema_pieces/            # Modular schema definitions
│       ├── autotag.json
│       ├── curation.json
│       ├── data_description.json  # CSV dialect, field mapping, field units
│       ├── figure_description.json
│       ├── minimum_echemdb.json
│       ├── source.json
│       ├── source_data.json       # Combines all pieces for source data files
│       ├── system.json
│       ├── experimental/
│       ├── general/              # Reusable types (quantity, url, etc.)
│       └── system/               # Electrolyte, electrode, cell schemas
│
├── examples/                      # Example YAML files
│   ├── file_schemas/             # Examples per schema type
│   │   ├── autotag.yaml
│   │   ├── minimum_echemebd.yaml
│   │   └── source_data.yaml
│   └── objects/                  # Examples of individual objects
│
├── tests/                         # Test data
│   ├── simple_test.yaml
│   ├── example_metadata.yaml
│   └── from_csv_example.csv
│
└── generated/                     # Project outputs (gitignored)
```

## Usage Examples

### Basic Flattening
```python
from mdstools.metadata.metadata import Metadata

metadata = Metadata(data)
flattened = metadata.flatten()
flattened.to_excel('output.xlsx')
```

### With Enrichment
```python
from mdstools.metadata.enriched_metadata import EnrichedFlattenedMetadata

metadata = Metadata(data)
flattened = metadata.flatten()
enriched = EnrichedFlattenedMetadata(flattened.rows, schema_dir='schemas')
enriched.to_excel('output.xlsx')
```

### Multi-Sheet Excel
```python
enriched.to_excel('output.xlsx', separate_sheets=True)
```

### Using CLI
```bash
mdstools flatten tests/example_metadata.yaml --output-dir generated
# or via pixi:
pixi run flatten tests/example_metadata.yaml --output-dir generated
```

## Testing

### Run All Tests
```bash
pixi run test                # Run all tests (doctests + comprehensive + resolved schemas)
pixi run doctest             # Run doctests only
pixi run test-comprehensive  # Run integration tests only
pixi run test-resolved-schemas  # Run resolved schema snapshot tests
```

### Resolve & Validate Schemas
```bash
pixi run resolve-schemas         # Generate schemas from LinkML
pixi run generate-schemas        # Same as resolve-schemas
pixi run generate-models         # Generate Pydantic models from LinkML
pixi run generate-all            # Generate both schemas and models
pixi run update-expected-schemas # Update expected baselines after intentional changes
pixi run validate                # Validate example YAMLs against schemas
pixi run diff-schemas            # Show diffs between expected and resolved schemas
```

### Run Conversion
```bash
mdstools flatten tests/example_metadata.yaml
# or via pixi:
pixi run flatten tests/example_metadata.yaml
```

## Future Enhancements

### Schema Resolution Strategy
- **Current**: Schemas generated from LinkML via `pixi run generate-schemas`
- **For Releases**: Use `pixi run resolve-schemas` to create single distributable schema files when creating GitHub releases/tags
  - This will make it easier for end users to have a single, self-contained schema file
  - The resolved schema should be included in the release assets

### Potential Additions
- **Validation**: Check if values match schema constraints (types, enums, required fields, patterns)
  - Could run validation on Excel sheets before converting back to YAML
  - Provide clear error messages for schema violations
- **Auto-completion**: Generate dropdown lists in Excel for enum fields
  - Use Data Validation feature in Excel with enum values from schemas
- **Excel Templates**: Pre-populate Example column values as Excel comments or lighter-colored text
- **GUI**: Simple interface for non-technical users (if needed)

## Backlog Ideas

- Clarify which columns are required when loading enriched Excel (Number/Key/Value)

### Schema Enhancement
To improve enrichment coverage beyond current ~14%:
- Add more `description` fields to all properties in JSON schemas
- Add `example` values for all primitive fields
- Consider adding `title` fields for human-friendly property names
- Document common patterns and best practices in schema comments

## Testing & Maintenance Notes

### Test Structure
- `pixi run test` - Runs all tests (doctests + comprehensive + resolved schema snapshots)
- `pixi run doctest` - Runs doctests in mdstools modules
- `pixi run test-comprehensive` - Runs integration tests (6 tests)
- `pixi run test-resolved-schemas` - Snapshot tests comparing resolved schemas against `schemas/expected/`
- Test outputs go to `tests/generated/` (gitignored)

### CLI Usage
- `mdstools flatten <yaml_file>` / `pixi run flatten <yaml_file>` - Flatten YAML to Excel/CSV
- `mdstools unflatten <file>` / `pixi run unflatten <file>` - Unflatten Excel/CSV back to YAML
- Click-based CLI registered as `mdstools` entry point via `[project.scripts]`
- Follows unitpackage's entrypoint pattern (`@click.group` + `@click.command` + `cli.add_command()`)
- Doctests in command docstrings discovered via `__test__` dict and pytest-doctestplus
- Outputs to `generated/` directory by default

### File Organization Decisions
- **Generated folders**:
  - `/generated` - Project outputs (user-facing conversions)
  - `/tests/generated` - Test outputs
  - Both excluded via `.gitignore`
- **Test files**:
  - `tests/simple_test.yaml` - Small test data for automated tests
  - `tests/example_metadata.yaml` - Comprehensive example for demos
- **Resolved schemas**: Generated from LinkML into `schemas/` and checked against `schemas/expected/` in CI
  - `mdstools/schema/generate_from_linkml.py` is the main generator
  - `mdstools/schema/resolver.py` kept for reference and legacy release workflows
  - `mdstools/schema/update_expected_schemas.py` updates expected baselines
  - Schemas generated: autotag, minimum_echemdb, source_data, svgdigitizer, echemdb_package, svgdigitizer_package

## Known Limitations

1. **Enrichment Coverage**: Currently ~14% on test data
   - Limited by how many descriptions/examples are in the JSON schemas
   - Not a code limitation - add more to schemas to improve

2. **Array Item Handling**:
   - Array items get lettered identifiers (1.1.a, 1.1.b)
   - Description/Example come from the array schema, not individual items
   - Works correctly but could be confusing for users

3. **Excel Limitations**:
   - Very large nested structures may create unwieldy Excel sheets
   - No built-in validation yet (users can enter invalid data)

## Conclusion

This implementation successfully provides:
1. ✅ Robust YAML ↔ Tabular conversion
2. ✅ Schema-based enrichment with descriptions/examples
3. ✅ Multiple export formats (CSV, Excel single/multi-sheet, Markdown)
4. ✅ Comprehensive test coverage (71 tests total: 64 doctests + 6 comprehensive + 1 snapshot)
5. ✅ Clean, documented, maintainable code
6. ✅ CLI interface via pixi tasks
7. ✅ Proper file organization and gitignore setup
8. ✅ Unflatten + validation (Excel/CSV → YAML with schema validation)
9. ✅ Schema snapshot testing (generated schemas vs expected baselines)
10. ✅ Data description schema (dialect, field mapping, field units) for source data files
11. ✅ LinkML as single source of truth (JSON Schema + Pydantic generated)
12. ✅ Pydantic-based validation with `validate_with_pydantic()`

The system is ready for users to:
- Generate Excel templates from YAML examples
- Fill out metadata with helpful descriptions and examples
- Convert completed Excel sheets back to YAML
- Validate metadata against JSON schemas

Schema types supported:
- **autotag** - Complete auto-generated echemdb metadata
- **minimum_echemdb** - Minimum set for electrochemical data
- **source_data** - Source data files with data description (dialect, field mapping, units)
- **svgdigitizer** - Digitizer output metadata
- **echemdb_package / svgdigitizer_package** - Data packages
